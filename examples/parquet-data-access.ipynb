{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205edafb-0a35-4b3e-9d3a-4d91b0d70117",
   "metadata": {},
   "source": [
    "# How to Access STREAM Data on HydroShare in Parquet Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52637e-8646-4072-84e6-8c49bde9cb5f",
   "metadata": {},
   "source": [
    "The purpose of this notebooks is to demonstrate common patterns for access STREAM data on HydroShare. The specific data we're going to access is stored in the Parquet file format which enables us to subset portions of the large STREAM catalog efficiently.\n",
    "\n",
    "First we'll need to authenticate with HydroShare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264324a-6704-4fa3-9f91-82945457fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import S3hsclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95777100-9d49-41cb-86fb-79907fbd9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = S3hsclient.S3HydroShare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58a272-3a4f-4985-a245-75fcf5a51516",
   "metadata": {},
   "source": [
    "MRB data exists in HydroShare at: https://hydroshare.org/resource/9fc3a923419640729b1606f0e64bd288/. To load these data, we'll use the HydroShare Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5578d71-b7ba-4152-8b30-28b0df2bc883",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_id = '9fc3a923419640729b1606f0e64bd288'\n",
    "resource = hs.resource(resource_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa8989-411c-4895-8cd6-7b67da0df2be",
   "metadata": {},
   "source": [
    "This resource contains a number of files that can be loaded for analysis. To see which files are available we'll use the `ls` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4dc44-2dde-428c-a6bc-b0f920b1ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource.s3_ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e3ed6-157b-4efd-8ba0-64b19c361fe8",
   "metadata": {},
   "source": [
    "## Accessing Data using Scientific Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb91029-6e5e-447a-b779-b87acaf78e21",
   "metadata": {},
   "source": [
    "Load the streamflow dataset and inspect the data. This can be done using several common scientific Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f666d-eb0e-4603-984b-497192bb7f50",
   "metadata": {},
   "source": [
    "### Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3152684-7cff-4571-989e-b88a6b223909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1060aaa-4580-4325-aa95-23eece7af9cb",
   "metadata": {},
   "source": [
    "The following example loads the entire record of streamflow data for the Mississippi River Basin. This includes over 100,000,000 records from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1168ee0-8af6-4fda-92c8-55a6476bfef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and plot data for one location using Pandas\n",
    "\n",
    "df = pandas.read_parquet(\n",
    "    'tonycastronova/9fc3a923419640729b1606f0e64bd288/data/contents/streamflow.parquet',\n",
    "    filesystem=hs.get_s3_filesystem()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249567c-ea88-4fb4-bd41-ba786102d64f",
   "metadata": {},
   "source": [
    "Calculate the number of gauges in the data that was returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1c8d1-867e-4368-ad70-d7e246c53bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_count = len(df.STREAM_ID.unique())\n",
    "print(f'{gauge_count} gauges exist in data returned from HydroShare')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c1d98-f09d-44d4-b079-ed7030c9fd00",
   "metadata": {},
   "source": [
    "Plot the streamflow for one of the STREAM sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b615e-e041-4d67-ab29-1ec053c1b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.STREAM_ID == 'STREAM-gauge-3010'].Q_m3s.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d69af19-6d26-48eb-acab-c80ce87f00a9",
   "metadata": {},
   "source": [
    "### Dask DataFrame\n",
    "\n",
    "The advantage of using a Dask DataFrame instead of a Pandas DataFrame is that it provides lazy loading and delayed computations. This is especially beneficial when analyzing large amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd5bae-80b5-4876-a3b3-274243da0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed60116-afe3-4214-8aac-da5c506f0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and plot data for one location using Dask\n",
    "\n",
    "# Create a dataset pointing to the files\n",
    "ds = dd.read_parquet('tonycastronova/9fc3a923419640729b1606f0e64bd288/data/contents/streamflow.parquet',\n",
    "                     filesystem=hs.get_s3_filesystem() \n",
    "                    )\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e7afa-80c6-4039-8e3f-3a24b3f096ef",
   "metadata": {},
   "source": [
    "Notice how fast this query was. This is because data has not been downloaded yet. We can perform our filtering and other computation before downloading any data, which means that we will not be downloading unnecessary data. For instance, we can plot the same data as above in the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b155a2e-e0ed-4627-9b5d-d632623776ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the dataset and filter for the 'STREAM-gauge-3010' location, then\n",
    "# return only DateTime and Q_m3s variables.\n",
    "dat = ds[ds.STREAM_ID == 'STREAM-gauge-3010'][['DateTime', 'Q_m3s']]\n",
    "\n",
    "# Perform additional computations/filtering here\n",
    "# ....\n",
    "# ....\n",
    "\n",
    "# tell dask to perform the computations\n",
    "dat = dat.compute()\n",
    "\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cb7a1-9f77-430c-9963-014a074e4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "dat.Q_m3s.plot();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
